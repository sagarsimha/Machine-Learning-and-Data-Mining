{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Class ML:IV\n",
    "## Ankit Satpute, 120825 ; Hsueh Wei, 120820; Sagar Nagaraj Simha, 120797 - (M.Sc. CS4DM) - Group 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import random\n",
    "import email\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starter Code\n",
    "\n",
    "def iterate_emails(tar_path):\n",
    "    \"\"\"Extract individual email messages from the tar file located at \n",
    "       tar_path. Returns a generator object. \"\"\"\n",
    "    tar = tarfile.open(tar_path)\n",
    "    emails = (f for f in tar if f.isfile())\n",
    "    for info in emails:\n",
    "        f = tar.extractfile(info)\n",
    "        ## parse contents of compressed file into an Email-object:\n",
    "        msg = email.message_from_binary_file(f)\n",
    "        yield msg\n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "def mail_text(msg):\n",
    "    \"\"\"Decode and extract the headers and body from the Email message \n",
    "       object given as msg, and return them as a single string. \"\"\"\n",
    "    headers = []\n",
    "    for k, v in msg.items():\n",
    "        headers.append(k)\n",
    "        if type(v) is str:\n",
    "            headers.append(v)\n",
    "    text_parts = (p for p in msg.walk() \n",
    "                  if p.get_content_type().startswith('text'))\n",
    "    contents = []\n",
    "    for txt in text_parts:\n",
    "        charset = txt.get_content_charset()\n",
    "        try:\n",
    "            ## decode MIME encoding\n",
    "            payload = txt.get_payload(decode=True)\n",
    "            try:\n",
    "                payload = payload.decode(charset)\n",
    "            except:\n",
    "                ## if the charset from the header doesn't work, force UTF-8\n",
    "                payload = payload.decode('utf-8', 'replace')\n",
    "            contents.append(payload)\n",
    "        except:\n",
    "            contents.append(txt.get_payload())\n",
    "    return \" \".join(headers + contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1397, 1401]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_email_text(path):\n",
    "    email_list = list(iterate_emails(path))\n",
    "    text_content = []\n",
    "    for msgs in email_list:\n",
    "        a= mail_text(msgs)\n",
    "        text_content.append(a)\n",
    "    return text_content\n",
    "\n",
    "#Files are downloaded first and then used for extracting emails from them\n",
    "ham = get_email_text('20030228_easy_ham_2.tar.bz2')\n",
    "spam = get_email_text('20050311_spam_2.tar.bz2')\n",
    "[len(spam),len(ham)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1397, 1401]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function tokenize splits emails into tokens and in this case we are getting \n",
    "# rid of everything else except alphanumeric characters in order to filter our data in begining\n",
    "def tokenize(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "#using function tokenize to create list of words from each email\n",
    "\n",
    "def tokenize_ham_spam(jam):\n",
    "    temp1 = []\n",
    "    for msgs in jam:\n",
    "        temp1.append(tokenize(msgs))\n",
    "    return temp1\n",
    "\n",
    "#Now onwards this will be our original collection of Ham and spam messages\n",
    "\n",
    "ham_re = tokenize_ham_spam(ham)\n",
    "spam_re = tokenize_ham_spam(spam)\n",
    "[len(spam),len(ham)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'test', 'string']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"this is a test string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns words as 'keys' and their occurance as 'values' in dictionaries\n",
    "\n",
    "def word_count(words):\n",
    "    c = Counter()\n",
    "    for l in words:\n",
    "        c.update(l)\n",
    "    return dict(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class implements naive bayes spam filter \n",
    "\n",
    "class SpamClassifier(object):\n",
    "    # constructor takes 2 parameters i.e. spam and ham which are already split into their constituent tokens\n",
    "    # then we use word_count to calculate no. of occurances of word \n",
    "    # we create total list of words from spam and ham in order to calculate P(wi/spam) for all words\n",
    "    \n",
    "    def __init__(self, spam, ham):\n",
    "        spam_words = word_count(spam)\n",
    "        no_s = len(spam)\n",
    "        ham_words = word_count(ham)\n",
    "        no_h = len(ham)\n",
    "        total_words = dict()\n",
    "        total_words.update(ham_words)\n",
    "        total_words.update(spam_words)\n",
    "        self.prob_word_provi_spam = dict()\n",
    "        \n",
    "        # we implement formula given in the article (4(a)) for assigning probabilities based on occurances\n",
    "        # then we save all the assigned probabilities into the dict so that we can use it effectively while calculating P(spam/ word)\n",
    "        \n",
    "        for word in total_words.keys():\n",
    "            g = b = 0\n",
    "            if word in spam_words:\n",
    "                b = 2 * spam_words[word]\n",
    "            if word in ham_words:\n",
    "                g = ham_words[word]\n",
    "            if (g + b > 5):\n",
    "                prob = max(0.01, min(0.99, float(min(1, b/no_s) / (min(1, g/no_h) + min(1, b/no_s)))))\n",
    "                self.prob_word_provi_spam[word] = prob\n",
    "        \n",
    "        #this represent probability of being spam which simply re[resent no of spam messages in total messages\n",
    "        self.prob_spam = no_s / (no_s + no_h)\n",
    "\n",
    "    # this is method predict for calculating prob such that new word is spam or not\n",
    "    def predict(self, test_words):\n",
    "        \n",
    "        # we create dict of all the test email words in order to calculate interesting words\n",
    "        # where interesting is measured by how far their spam probability is from a neutral .5\n",
    "        # if word is not found in P(word/spam) then we assign 0.1 because abs(0.5-0.4)=0.1 always\n",
    "        \n",
    "        test = dict()\n",
    "        for word in test_words:\n",
    "            if word in self.prob_word_provi_spam:\n",
    "                test[word] = abs(0.5 - self.prob_word_provi_spam[word])\n",
    "            else:\n",
    "                test[word] = abs(0.1)\n",
    "        \n",
    "        # As mentioned in the article 4(a)\n",
    "        # we need to sort all the words because we are only interested in first 15 tokens \n",
    "        \n",
    "        key_words = {k: v for k, v in sorted(test.items(), key=lambda i: i[1], reverse=True)}\n",
    "    \n",
    "        # now we calculate final prob whether given word is spam or not using baye's formula\n",
    "        # note that the word from test mail which is not in our train set prob, we assign prob = 0.4 \n",
    "        # also, prob(word/ham) = 1 - prob(word/spam) as its ither ham or spam\n",
    "        \n",
    "        word_giv_spam = 1\n",
    "        word_giv_ham = 1\n",
    "        for word in list(key_words.keys())[:15]:\n",
    "            if word in self.prob_word_provi_spam:\n",
    "                word_giv_spam *= self.prob_word_provi_spam[word]\n",
    "                word_giv_ham *= (1 - self.prob_word_provi_spam[word])\n",
    "            else:\n",
    "                self.prob_word_provi_spam[word] = 0.4\n",
    "                word_giv_spam *= self.prob_word_provi_spam[word]\n",
    "                word_giv_ham *= (1 - self.prob_word_provi_spam[word])   \n",
    "        final_prob = (self.prob_spam *word_giv_spam) / (self.prob_spam*word_giv_spam + self.prob_spam*word_giv_ham)\n",
    "        return final_prob\n",
    "        \n",
    "    # this function we implemented in order to return whether the message is spam (True) or not (False) depending on the \n",
    "    # probability we calculate in 'predict' method\n",
    "    # threashold is taken as 0.9 (according to aricle 4(a))\n",
    "    \n",
    "    def check_if_spam(self, text, thr=0.9):\n",
    "        x = self.predict(text)\n",
    "        tem = False\n",
    "        if x > thr:\n",
    "            tem = True\n",
    "        return tem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11636363636363642"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = SpamClassifier(spam = [[\"this\", \"is\", \"spam\"], [\"more\", \"spam\"]],\n",
    "ham = [[\"this\", \"is\", \"ham\"], [\"more\", \"ham\"]])\n",
    "cls.predict(['is', \"this\", \"spam\", \"or\", \"not\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   predicted class “spam”  predicted class “ham”\n",
      "true class “spam”                     100                      0\n",
      "true class “ham”                        4                     96\n"
     ]
    }
   ],
   "source": [
    "# randomly shuffle the messages \n",
    "\n",
    "random.shuffle(ham_re)\n",
    "random.shuffle(spam_re)\n",
    "\n",
    "# We select 100 random examples from each of the spam and ham classes \n",
    "# to use as a validation set, and exclude them from the training data\n",
    "\n",
    "spam_train = spam_re[100:]\n",
    "spam_valida = spam_re[:100]\n",
    "ham_train = ham_re[100:]\n",
    "ham_valida = ham_re[:100]\n",
    "\n",
    "cls_1 = SpamClassifier(spam_train, ham_train)\n",
    "a = b= c= d = 0\n",
    "for data in spam_valida:\n",
    "    if cls_1.check_if_spam(data) is True:\n",
    "        a +=1\n",
    "    else:\n",
    "        b +=1\n",
    "\n",
    "for data in ham_valida:\n",
    "    if cls_1.check_if_spam(data) is True:\n",
    "        c +=1\n",
    "    else:\n",
    "        d +=1\n",
    "data = [[a, b], [c, d]] \n",
    "df = pd.DataFrame(data, columns = ['predicted class “spam”', 'predicted class “ham”'], index= ['true class “spam”', 'true class “ham”'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three words or tokens that are the \"strongest\" evidence that an email is \"spam\" are:  ['YOU', 'span', 'weight']\n",
      "three words or tokens that are the \"strongest\" evidence that an email is \"ham\" are:  ['rpm', 'zzzlist', 'freshrpms']\n",
      "three words or tokens that are the \"Wekaest\" evidence that an email is \"spam\" are:  ['rpm', 'zzzlist', 'freshrpms', 'egwn', 'auth02', 'EGWN']\n",
      "three words or tokens that are the \"weakest\" evidence that an email is \"ham\" are:  ['YOU', 'span', 'weight', 'crackmice', 'insurance', 'ns']\n"
     ]
    }
   ],
   "source": [
    "# We train our classifier on the entire dataset (including the validation set) and examine the conditional\n",
    "# spam probabilities for individual words that our classifier computes during training and print results asked in 4(f)\n",
    "# We create two dicts based on prob of words being spam i.e. first 3 words in the list with \"greater prob on begining\" \n",
    "# will be tokens as strongest evidance as spam\n",
    "# and those with lowest prob will be strongest evidance as ham\n",
    "\n",
    "cls_2 = SpamClassifier(spam_re, ham_re)\n",
    "prob = cls_2.prob_word_provi_spam\n",
    "prob_sorted = {k: v for k, v in sorted(prob.items(), key=lambda item: item[1])}\n",
    "prob_sorted_rever = {k: v for k, v in sorted(prob.items(), key=lambda item: item[1], reverse = True)}\n",
    "\n",
    "first_3_ham = {k: prob_sorted[k] for k in list(prob_sorted)[:3]}\n",
    "first_3_spam = {k: prob_sorted_rever[k] for k in list(prob_sorted_rever)[:3]}\n",
    "print('three words or tokens that are the \"strongest\" evidence that an email is \"spam\" are: ', list(first_3_spam.keys()))\n",
    "print('three words or tokens that are the \"strongest\" evidence that an email is \"ham\" are: ', list(first_3_ham.keys()))\n",
    "\n",
    "first_6_weak_spam = {k: prob_sorted[k] for k in list(prob_sorted)[:6]}\n",
    "first_6_wek_ham = {k: prob_sorted_rever[k] for k in list(prob_sorted_rever)[:6]}\n",
    "print('three words or tokens that are the \"Wekaest\" evidence that an email is \"spam\" are: ', list(first_6_weak_spam.keys()))\n",
    "print('three words or tokens that are the \"weakest\" evidence that an email is \"ham\" are: ', list(first_6_wek_ham.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   predicted class “spam”  predicted class “ham”\n",
      "true class “spam”                     500                      1\n",
      "true class “ham”                      229                     22\n"
     ]
    }
   ],
   "source": [
    "# Testing the trained classifier using the additional datasets and printing misclassification results\n",
    "# Datasets are already considered as downloaded\n",
    "\n",
    "ham_new = get_email_text('20030228_hard_ham.tar.bz2')\n",
    "spam_new = get_email_text('20030228_spam.tar.bz2')\n",
    "ham_re_1 = tokenize_ham_spam(ham_new)\n",
    "spam_re_1 = tokenize_ham_spam(spam_new)\n",
    "a = b= c= d = 0\n",
    "for data in spam_re_1:\n",
    "    if cls_1.check_if_spam(data) is True:\n",
    "        a +=1\n",
    "    else:\n",
    "        b +=1\n",
    "\n",
    "for data in ham_re_1:\n",
    "    if cls_1.check_if_spam(data) is True:\n",
    "        c +=1\n",
    "    else:\n",
    "        d +=1\n",
    "data = [[a, b], [c, d]] \n",
    "df = pd.DataFrame(data, columns = ['predicted class “spam”', 'predicted class “ham”'], index= ['true class “spam”', 'true class “ham”'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Refernces\n",
    "# https://stackoverflow.com/questions/7971618/python-return-first-n-keyvalue-pairs-from-dict\n",
    "# https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer\n",
    "# https://www.uni-weimar.de/en/media/chairs/computer-science-department/webis/teaching/previous-semesters/ws-201819/machine-learning/\n",
    "# https://www.guru99.com/python-lambda-function.html#3\n",
    "# https://blog.softhints.com/python-get-first-elements-dictionary/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
