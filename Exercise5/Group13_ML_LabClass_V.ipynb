{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Class ML:V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ankit Satpute, 120825 ; Hsueh Wei, 120820; Sagar Nagaraj Simha, 120797 - (M.Sc. CS4DM) - Group 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - Perceptron Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separable_dataset():\n",
    "    X, C = make_classification(300, 2, 2, 0, 0, 2, 1,\n",
    "                                class_sep=30, flip_y=0,\n",
    "                                hypercube=False, random_state=2)\n",
    "    X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "    C = 2*C - 1\n",
    "    D = (X, C)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = separable_dataset() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D[0] is X matrix of size nx3\n",
    "#D[1] is C matrix of size nx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 3.94931935, 6.85201923])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[0][0] # First sample in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[1][0] #First sample in C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_select(D):\n",
    "    # generates a random integer number between 0 and len(D)-1, inclusive\n",
    "    i = random.randint(0, len(D[0])-1)\n",
    "    #print(i)\n",
    "    # returns the tuple of index i in D \n",
    "    return ((D[0][i],D[1][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.        , -6.72592941,  6.94875964]), -1)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_select(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 0, 0], -1)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test case\n",
    "random_select(([[1,0,0]], [-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convergence(D, w):\n",
    "    #Picks X and C from D\n",
    "    X = D[0]\n",
    "    C = D[1] \n",
    "    y = np.sign(np.dot(X,w)) #wT.x\n",
    "    equal = np.array_equal(y, C) #Returns True if two numpy arrays are equal, False otherwise \n",
    "    return equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test cases\n",
    "X_t = np.array([[1, 0, 0]])\n",
    "C_t = np.array([1])\n",
    "w_t = np.array([1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convergence(D=(X_t, C_t), w=w_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convergence(D=(X_t, C_t), w=-w_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt(D,eta, tmax=300):\n",
    "    # Perceptron Training Algorithm\n",
    "    t = 0\n",
    "    delta_w = 0\n",
    "    #Initialize weights with zeros\n",
    "    #w = np.zeros(3)\n",
    "    #Initialize weights with a value between 0 and 1 drawn from an uniform distribution\n",
    "    w = np.array([random.uniform(0, 1),random.uniform(0, 1),random.uniform(0, 1)])\n",
    "    #Repeat until convergence or max iterations\n",
    "    while( (not convergence(D,w)) and t<=tmax):\n",
    "        t+=1\n",
    "        (x,c) = random_select(D)\n",
    "        error = c - np.sign(np.dot(x,w))\n",
    "        delta_w = eta * error * x\n",
    "        w+=delta_w\n",
    "    return w # return weight vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclass_rate(D, w):\n",
    "    # Misclassification rate is calculated as (number of wrong classifications / Total number of samples)\n",
    "    X = D[0]\n",
    "    C = D[1] \n",
    "    y = np.sign(np.dot(X,w)) #threshold(wT.x)\n",
    "    comparison = (C==y) #Returns an array with True/False for element wise equality \n",
    "    missclassification_rate = (np.size(comparison) - np.count_nonzero(comparison)) / np.size(comparison) #Counts number of False values \n",
    "    return missclassification_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missclassification rate is 0.0\n"
     ]
    }
   ],
   "source": [
    "#Test case\n",
    "D = separable_dataset()\n",
    "w = pt(D, eta=0.1, tmax=300)\n",
    "print('Missclassification rate is', misclass_rate(D, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonseparable_dataset():\n",
    "    X, C = make_classification(300, 2, 2, 0, 0, 2, 1,\n",
    "                                class_sep=10, flip_y=0.1,\n",
    "                                hypercube=False, random_state=2)\n",
    "    X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "    C = 2*C - 1\n",
    "    return (X, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PT for 1000 runs\n",
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "PT for 1000 runs completed\n"
     ]
    }
   ],
   "source": [
    "missclassification_rate_ = []\n",
    "#Performs for 1000 runs\n",
    "print('Running PT for 1000 runs')\n",
    "for i in range(1000):\n",
    "    D = nonseparable_dataset()\n",
    "    w = pt(D, eta=0.1, tmax=300)\n",
    "    missclassification_rate_.append(round(misclass_rate(D, w),3))\n",
    "    print('.',end='')\n",
    "    #print('run:',i, '::misclassification rate:', round(misclass_rate(D, w),3))\n",
    "print('\\nPT for 1000 runs completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclasssification rate statistics - PT: \n",
      " Minimum -  0.07 \n",
      " Average -  0.139 \n",
      " Maximum -  0.513\n"
     ]
    }
   ],
   "source": [
    "missclassification_rate_s = pd.Series(missclassification_rate_)\n",
    "stats = missclassification_rate_s.describe()\n",
    "print('Misclasssification rate statistics - PT:', '\\n', 'Minimum - ', stats['min'], '\\n', 'Average - ', round(stats['mean'],3), '\\n', 'Maximum - ', stats['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missclassification_rate_s.plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgd(D,eta, tmax=300):\n",
    "    t = 0\n",
    "    #Initialize weights with zeros\n",
    "    #w = np.zeros(3)\n",
    "    #Initialize weights with a value between 0 and 1 drawn from an uniform distribution\n",
    "    w = np.array([random.uniform(0, 1),random.uniform(0, 1),random.uniform(0, 1)])\n",
    "    #Repeat until convergence or max iterations\n",
    "    while( (not convergence(D,w)) and t<=tmax):\n",
    "        t+=1\n",
    "        delta_w = 0\n",
    "        for k in range(len(D[0])):\n",
    "            x = D[0][k]\n",
    "            c = D[1][k]\n",
    "            error = c - np.sign(np.dot(x,w))\n",
    "            delta_w+= eta * error * x\n",
    "        w+=delta_w\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BGD for 1000 runs\n",
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "BGD for 1000 runs completed\n"
     ]
    }
   ],
   "source": [
    "missclassification_rate_bgd = []\n",
    "# For 1000 runs\n",
    "print('Running BGD for 1000 runs')\n",
    "for i in range(1000):\n",
    "    D_ = nonseparable_dataset()\n",
    "    w_ = bgd(D_, eta=0.01, tmax=300)\n",
    "    #print(w_)\n",
    "    missclassification_rate_bgd.append(round(misclass_rate(D_, w_),3))\n",
    "    print('.',end='')\n",
    "    #print('run:',i, '::misclassification rate BGD:', round(misclass_rate(D_, w_),3))\n",
    "print('\\nBGD for 1000 runs completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclasssification rate statistics - BGD: \n",
      " Minimum -  0.07 \n",
      " Average -  0.13953200000000002 \n",
      " Maximum -  0.487\n"
     ]
    }
   ],
   "source": [
    "missclassification_rate_s_bgd = pd.Series(missclassification_rate_bgd)\n",
    "#missclassification_rate_s\n",
    "stats_bgd = missclassification_rate_s_bgd.describe()\n",
    "print('Misclasssification rate statistics - BGD:', '\\n', 'Minimum - ', stats_bgd['min'], '\\n', 'Average - ', stats_bgd['mean'], '\\n', 'Maximum - ', stats_bgd['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missclassification_rate_s_bgd.plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison -  \n",
    "### 1) The performance seem to be strongly dependent on how the weight vectors are initialized. \n",
    "In case of a weights chosen from a uniform random number between 0 and 1,\n",
    "\n",
    "Misclasssification rate statistics - PT: \n",
    " Minimum -  0.07,\n",
    " Average -  0.139, \n",
    " Maximum -  0.513.\n",
    "Misclasssification rate statistics - BGD: \n",
    " Minimum -  0.07, \n",
    " Average -  0.13953200000000002, \n",
    " Maximum -  0.487.\n",
    "\n",
    "We can observe that the minimum and average misclassification rate is similar but in terms of the maximum value, BGD performs better than Pt with a lower misclassification rate.  \n",
    "\n",
    "\n",
    "In case of a weights chosen as a zero vector [0,0,0] ,\n",
    "Misclasssification rate statistics - PT: \n",
    " Minimum -  0.07, \n",
    " Average -  0.137, \n",
    " Maximum -  0.593.\n",
    "Misclasssification rate statistics - BGD: \n",
    " Minimum -  0.08333, \n",
    " Average -  0.83333333, \n",
    " Maximum -  0.083333.\n",
    " \n",
    "Here, we observe that the BGD algorithm saturates to 0.083 throught all the runs since each time the weight vectors are initialized to zero and the algorithm goes through each sample.\n",
    "  \n",
    "### 2) A histogram analysis shows that the BGD has lower variance from the mean compared to Pt\n",
    "### 3) The BGD algorithm takes a longer time compared to Pt since it goes through all the training data in each iteration while Pt goes through a random sample in each iteration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
